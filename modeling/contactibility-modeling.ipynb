{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import datetime\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize \n",
    "\n",
    "import civis\n",
    "import civis.io\n",
    "from civis.futures import CivisFuture\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, RandomForestRegressor\n",
    "from civis.ml import ModelPipeline\n",
    "\n",
    "import uuid\n",
    "import json\n",
    "from pprint import pprint\n",
    "import tempfile\n",
    "import concurrent.futures\n",
    "from concurrent.futures import wait\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Default feature lists for Rainbow Modeling Frame (each number corresponds to number of features)\n",
    "feature_table = civis.io.read_civis_sql(sql='select * from bernie_nmarchio2.feature_list order by sort_order asc', use_pandas = True, database='Bernie 2020')\n",
    "feature_list_large = list(feature_table[(feature_table['frame_large'] == 1)]['feature_name']) + ['state_code']\n",
    "feature_list_medium = list(feature_table[(feature_table['frame_medium'] == 1)]['feature_name']) + ['state_code']\n",
    "feature_list_small = list(feature_table[(feature_table['frame_small'] == 1)]['feature_name']) + ['state_code']\n",
    "\n",
    "additional_features = ['attempt_count', 'rank_1_is_cell_phone',\n",
    "                       'num_phones_1', 'num_phones_2', 'num_phones_3', 'num_phones_other']\n",
    "\n",
    "feature_list_large.extend(additional_features)\n",
    "feature_list_medium.extend(additional_features)\n",
    "feature_list_small.extend(additional_features)\n",
    "\n",
    "table_columns = civis.io.read_civis_sql(\n",
    "    sql=f'''select ordinal_position as position, column_name, data_type \n",
    "    from information_schema.columns \n",
    "    where table_name = 'rainbow_modeling_frame' and table_schema = 'bernie_data_commons' and column_name != 'person_id'\n",
    "    order by ordinal_position;''', use_pandas = True, database='Bernie 2020')\n",
    "\n",
    "#exclusion_list_466 = [e for e in list(table_columns['column_name']) if e not in feature_list_466] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_list_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# USER INPUT CELL\n",
    "\n",
    "# DV table parameters\n",
    "DATABASE = 'Bernie 2020'\n",
    "# Primary key in both the DV table and the Modeling Frame\n",
    "PRIMARY_KEY = 'person_id' \n",
    "# Table containing recoded Dependent Variables keyed to the PRIMARY_KEY\n",
    "DV_TABLE = 'bernie_cherdeman.contactibility_outcomes'\n",
    "# List of binarized dependent variables (accepts 1, 0, and null values) in DV_TABLE\n",
    "DV_LIST = [ 'pickup_first', 'pickup_last', 'pickup_ever', \n",
    "            'id_first','id_last', 'id_ever'] # 'pickup_first', 'id_first',\n",
    "\n",
    "\n",
    "# Modeling frame table parameters\n",
    "# Table containing covariates and keyed to PRIMARY_KEY\n",
    "MODELING_FRAME = 'bernie_cherdeman.contactibility_modeling_frame'\n",
    "# Columns in the Modeling Frame to exclude from feature list (i.e., strings or incomplete coverage)\n",
    "EXCLUSION_COLUMNS = ['jsonid','state_code','census_block_group_2010', 'person_id', 'us_region', \n",
    "                     'pickup_first', 'pickup_last', 'pickup_ever', 'id_first', 'id_last', 'id_ever']\n",
    "\n",
    "# Output table parameters\n",
    "# Schema to contain prediction tables\n",
    "SCHEMA = 'bernie_cherdeman'\n",
    "# String that will be concatenated in front of the output table's name\n",
    "PREFIX = 'getthru'\n",
    "\n",
    "# Sampling parameters\n",
    "# Non-response training data\n",
    "    # True: automatically select people not in DV_TABLE at random from Phoenix (assumes person_id is PRIMARY_KEY)\n",
    "    # False: automatically select people where the DV equals 0 from the DV_TABLE\n",
    "SAMPLE_FROM_PHOENIX = False\n",
    "# Number of non-response classes per target class (default is 2) \n",
    "CLASS_BALANCE = 2\n",
    "# Maximum number of targets to randomly sample from DV_TABLE\n",
    "MAX_TARGET_COUNT = 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datestamp = '{:%Y%m%d}'.format(datetime.date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counts of positive classes\n",
    "dv_sql_targets = \"\\n,\".join([\"sum({dv}) as {dv}\".format(dv=i) for i in DV_LIST])\n",
    "sql_collapse_targets = f\"\"\"select {dv_sql_targets} from {DV_TABLE};\"\"\"\n",
    "sql_count_targets = civis.io.read_civis_sql(sql_collapse_targets, DATABASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pickup_first',\n",
       "  'pickup_last',\n",
       "  'pickup_ever',\n",
       "  'pickup_last',\n",
       "  'id_last',\n",
       "  'id_ever'],\n",
       " ['202624', '236411', '341585', '236411', '257413', '337340']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_count_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determing training table proportion of positives to negatives (to avoid class imbalance problems)\n",
    "# sample_share = []\n",
    "# for i in range(len(DV_LIST)):\n",
    "#     if int(sql_count_targets[1][i]) > MAX_TARGET_COUNT:\n",
    "#         sql_count_targets[1][i] = MAX_TARGET_COUNT\n",
    "#     u = round(int(sql_count_targets[1][i])*CLASS_BALANCE)\n",
    "#     sample_share.append(u)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(len(DV_LIST)):\n",
    "#     dv_item = DV_LIST[i]\n",
    "#     random_sample = sample_share[i]\n",
    "#     if SAMPLE_FROM_PHOENIX is True:\n",
    "#         zero_sample = f'''(select p.person_id, 0 as {dv_item} from phoenix_analytics.person p left join (select person_id from {DV_TABLE}) d on p.person_id = d.person_id where d.person_id is null and is_deceased = false and reg_record_merged = false and reg_on_current_file = true and reg_voter_flag = true order by random() limit {random_sample})'''   \n",
    "#     if SAMPLE_FROM_PHOENIX is False:\n",
    "#         zero_sample = f'''(select {PRIMARY_KEY}, {dv_item} from {DV_TABLE} where {dv_item} = 0 order by random() limit {random_sample})'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one down\n",
      "one down\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'select * \\n   from {MODELING_FRAME}\\n   where pickup_strat < 2001'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training tables\n",
    "#for i, dv_item in enumerate(DV_LIST):\n",
    "#     if (int(sql_count_targets[1][i])*3) <= 1000:\n",
    "    #feature_select = \"\\n,\".join([\"{feature}\".format(feature=f) for f in feature_list_small])\n",
    "#     if (int(sql_count_targets[1][i])*3) > 1000 & (int(sql_count_targets[1][i])*3) <= 2000:\n",
    "         \n",
    "    #feature_select = \"\\n,\".join([\"{feature}\".format(feature=f) for f in feature_list_medium])\n",
    "#     if (int(sql_count_targets[1][i])*3) > 2000:\n",
    "    #feature_select = \"\\n,\".join([\"{feature}\".format(feature=f) for f in feature_list_large])\n",
    "    #dv_item = DV_LIST[i]\n",
    "#print(dv_item)\n",
    "#     random_sample = sample_share[i]\n",
    "#     if SAMPLE_FROM_PHOENIX is True:\n",
    "#         zero_sample = f'''(select p.person_id, 0 as {dv_item} from phoenix_analytics.person p left join (select person_id from {DV_TABLE}) d on p.person_id = d.person_id where d.person_id is null and is_deceased = false and reg_record_merged = false and reg_on_current_file = true and reg_voter_flag = true order by random() limit {random_sample})'''   \n",
    "#     if SAMPLE_FROM_PHOENIX is False:\n",
    "#         zero_sample = f'''(select {PRIMARY_KEY}, {dv_item} from {DV_TABLE} where {dv_item} = 0 order by random() limit {random_sample})'''\n",
    "feature_select = \"\\n,\".join([\"{feature}\".format(feature=f) for f in feature_list_small])\n",
    "dvs = \"\\n,\".join([\"{dv}\".format(dv=d) for d in DV_LIST])\n",
    "\n",
    "training_sql1 = f\"\"\"DROP TABLE IF EXISTS {SCHEMA}.{PREFIX}_training_0 CASCADE;\n",
    "                       CREATE TABLE {SCHEMA}.{PREFIX}_training_0 DISTKEY({PRIMARY_KEY}) SORTKEY({PRIMARY_KEY}) AS (\n",
    "                           select * \n",
    "                           from (select {PRIMARY_KEY}, {dvs} \n",
    "                               from {DV_TABLE}\n",
    "                               where stratification < 2500)\n",
    "                           join (select {PRIMARY_KEY}, {feature_select}\n",
    "                                 from {MODELING_FRAME})\n",
    "                           using({PRIMARY_KEY})\n",
    "                           );\"\"\"\n",
    "\n",
    "training_sql2 = f\"\"\"DROP TABLE IF EXISTS {SCHEMA}.{PREFIX}_training_1 CASCADE;\n",
    "                       CREATE TABLE {SCHEMA}.{PREFIX}_training_1 DISTKEY({PRIMARY_KEY}) SORTKEY({PRIMARY_KEY}) AS (\n",
    "                           select * \n",
    "                           from (select {PRIMARY_KEY}, {dvs} \n",
    "                               from {DV_TABLE}\n",
    "                               where stratification between 2500 and 5000)\n",
    "                           join (select {PRIMARY_KEY}, {feature_select}\n",
    "                                 from {MODELING_FRAME})\n",
    "                           using({PRIMARY_KEY})\n",
    "                           );\"\"\"\n",
    "training = [training_sql1, training_sql2]\n",
    "\n",
    "for t in training:\n",
    "    create_training_sql = civis.io.query_civis(t, database=DATABASE)\n",
    "    create_training_sql.result().state\n",
    "    print(\"one down\")\n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"select * \n",
    "   from (select {PRIMARY_KEY}, {cd} \n",
    "       from {DV_TABLE}\n",
    "       where {dv_item}_strat < 5000)\n",
    "   join (select {PRIMARY_KEY}, {feature_select}\n",
    "         from {MODELING_FRAME})\n",
    "   using({PRIMARY_KEY});\"\"\"\n",
    "\n",
    "\"\"\"select * \n",
    "   from {MODELING_FRAME}\n",
    "   where pickup_strat < 2001\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'succeeded'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine training sets into view\n",
    "training_sql = f\"\"\"drop view if exists {SCHEMA}.{PREFIX}_training;\n",
    "                   create view {SCHEMA}.{PREFIX}_training as (\n",
    "                        (select * from {SCHEMA}.{PREFIX}_training_0)\n",
    "                        union\n",
    "                        (select * from {SCHEMA}.{PREFIX}_training_1)\n",
    "                    );\n",
    "                   \"\"\"\n",
    "combined_training_sql = civis.io.query_civis(training_sql, database=DATABASE)\n",
    "combined_training_sql.result().state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING >>> pickup_first\n",
      "TRAINING >>> pickup_last\n",
      "TRAINING >>> pickup_ever\n",
      "TRAINING >>> id_first\n",
      "TRAINING >>> id_last\n",
      "TRAINING >>> id_ever\n"
     ]
    }
   ],
   "source": [
    "# Train models\n",
    "train_list = []\n",
    "model_list = []\n",
    "\n",
    "for i, dv in enumerate(DV_LIST):\n",
    "    print('TRAINING >>> {}'.format(dv))\n",
    "    \n",
    "    exc_list = DV_LIST.copy()\n",
    "    exc_list.remove(dv)\n",
    "    \n",
    "    assert dv not in exc_list \n",
    "    \n",
    "    for m in ['random_forest_classifier', 'sparse_logistic']:\n",
    "    \n",
    "        name = f\"\"\"{dv}_{m}_{datestamp}\"\"\"\n",
    "        model = ModelPipeline(model=m,\n",
    "                              dependent_variable=dv,\n",
    "                              primary_key=PRIMARY_KEY,\n",
    "                              excluded_columns=EXCLUSION_COLUMNS,\n",
    "                              #calibration='sigmoid',\n",
    "                              model_name=name,\n",
    "                              memory_requested=15000#,\n",
    "                              #disk_requested=5\n",
    "                             )\n",
    "    \n",
    "        where_string = '{} is not null'.format(dv)\n",
    "        # Use \n",
    "        train = model.train(table_name=f\"\"\"{SCHEMA}.{PREFIX}_training\"\"\", \n",
    "                            database_name=DATABASE,\n",
    "                            sql_where=where_string#,\n",
    "                            #fit_params={'sample_weight': WEIGHT_VAR}\n",
    "                           )\n",
    "\n",
    "        model_list.append(model)\n",
    "        train_list.append(train) \n",
    "\n",
    "name = f\"\"\"multioutcome_rf_{datestamp}\"\"\"\n",
    "model = ModelPipeline(model='random_forest_classifier',\n",
    "                      dependent_variable=DV_LIST,\n",
    "                      primary_key=PRIMARY_KEY,\n",
    "                      excluded_columns=EXCLUSION_COLUMNS,\n",
    "                      #calibration='sigmoid',\n",
    "                      model_name=name,\n",
    "                      memory_requested=15000#,\n",
    "                      #disk_requested=5\n",
    "                     )\n",
    "\n",
    "train = model.train(table_name=f\"\"\"{SCHEMA}.{PREFIX}_training\"\"\", \n",
    "                    database_name=DATABASE,\n",
    "                    sql_where='''where pickup_first is not null \n",
    "                                 and pickup_last is not null\n",
    "                                 and pickup_ever is not null\n",
    "                                 and id_first is not null\n",
    "                                 and id_last is not null\n",
    "                                 and id_ever is not null'''#,\n",
    "                    #fit_params={'sample_weight': WEIGHT_VAR}\n",
    "                   )\n",
    "\n",
    "model_list.append(model)\n",
    "train_list.append(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = f\"\"\"multioutcome_rf_{datestamp}_v4\"\"\"\n",
    "model = ModelPipeline(model='random_forest_classifier',\n",
    "                      dependent_variable=DV_LIST,\n",
    "                      primary_key=PRIMARY_KEY,\n",
    "                      excluded_columns=EXCLUSION_COLUMNS,\n",
    "                      #calibration='sigmoid',\n",
    "                      model_name=name,\n",
    "                      memory_requested=15000#,\n",
    "                      #disk_requested=5\n",
    "                     )\n",
    "\n",
    "train = model.train(table_name=f\"\"\"{SCHEMA}.{PREFIX}_training\"\"\", \n",
    "                    database_name=DATABASE,\n",
    "                    sql_where='''pickup_first is not null \n",
    "                                 and pickup_last is not null\n",
    "                                 and pickup_ever is not null\n",
    "                                 and id_first is not null\n",
    "                                 and id_last is not null\n",
    "                                 and id_ever is not null'''#,\n",
    "                    #fit_params={'sample_weight': WEIGHT_VAR}\n",
    "                   )\n",
    "\n",
    "model_list.append(model)\n",
    "train_list.append(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job success\n",
      "Job success\n",
      "Job success\n",
      "Job success\n",
      "Job success\n",
      "Job success\n",
      "Job success\n",
      "Job success\n",
      "Job success\n",
      "Job success\n",
      "Job success\n",
      "Job success\n",
      "Job failure\n",
      "Job failure\n",
      "Job failure\n",
      "Job success\n"
     ]
    }
   ],
   "source": [
    "# Extract successful models\n",
    "model_output = model_list\n",
    "train_output = train_list\n",
    "\n",
    "jobs_list = []\n",
    "for t in train_output: \n",
    "    try:\n",
    "        if len(t.metadata['output']) > 0:  \n",
    "            jobs_list.append(t)\n",
    "            print('Job success')\n",
    "    except:\n",
    "        print('Job failure')\n",
    "        pass\n",
    "\n",
    "    \n",
    "model_output, train_output = zip(*((m, t) for m, t in zip(model_output, train_output) if t in jobs_list))\n",
    "model_output = list(model_output)\n",
    "train_output = list(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(jobs_list))\n",
    "print(len(model_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>dv</th>\n",
       "      <th>model</th>\n",
       "      <th>time_of_train_run</th>\n",
       "      <th>n_rows</th>\n",
       "      <th>n_features</th>\n",
       "      <th>auc</th>\n",
       "      <th>deciles</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>p_correct</th>\n",
       "      <th>pop_incidence_true</th>\n",
       "      <th>feature_list</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>run_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>206047980</th>\n",
       "      <td>59759537</td>\n",
       "      <td>pickup_first</td>\n",
       "      <td>random_forest_classifier</td>\n",
       "      <td>2020-01-30T16:22:35Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.769170</td>\n",
       "      <td>[0.07304526748971193, 0.11215640158753491, 0.1...</td>\n",
       "      <td>[[46731, 581], [19211, 1513]]</td>\n",
       "      <td>0.709095</td>\n",
       "      <td>[0.9877198173824823, 0.07300714147847906]</td>\n",
       "      <td>[0.6953965547651243, 0.30460344523487565]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206047993</th>\n",
       "      <td>59759544</td>\n",
       "      <td>pickup_first</td>\n",
       "      <td>sparse_logistic</td>\n",
       "      <td>2020-01-30T16:22:02Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.757209</td>\n",
       "      <td>[0.08597883597883597, 0.10157283551374394, 0.1...</td>\n",
       "      <td>[[42357, 4955], [12859, 7865]]</td>\n",
       "      <td>0.738168</td>\n",
       "      <td>[0.8952696990192763, 0.37951167728237795]</td>\n",
       "      <td>[0.6953965547651243, 0.30460344523487565]</td>\n",
       "      <td>[civis_2020_children_present, civis_2020_spani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206047997</th>\n",
       "      <td>59759550</td>\n",
       "      <td>pickup_last</td>\n",
       "      <td>random_forest_classifier</td>\n",
       "      <td>2020-01-30T16:22:42Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.752649</td>\n",
       "      <td>[0.09082892416225749, 0.1306776422166691, 0.18...</td>\n",
       "      <td>[[42671, 1037], [21519, 2809]]</td>\n",
       "      <td>0.668470</td>\n",
       "      <td>[0.9762743662487416, 0.11546366326866163]</td>\n",
       "      <td>[0.6424245987418425, 0.35757540125815745]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206048012</th>\n",
       "      <td>59759567</td>\n",
       "      <td>pickup_last</td>\n",
       "      <td>sparse_logistic</td>\n",
       "      <td>2020-01-30T16:22:07Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.745124</td>\n",
       "      <td>[0.09141681363903587, 0.12803175069822137, 0.1...</td>\n",
       "      <td>[[36374, 7334], [12840, 11488]]</td>\n",
       "      <td>0.703481</td>\n",
       "      <td>[0.8322046307312162, 0.47221308780006577]</td>\n",
       "      <td>[0.6424245987418425, 0.35757540125815745]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206048016</th>\n",
       "      <td>59759573</td>\n",
       "      <td>pickup_ever</td>\n",
       "      <td>random_forest_classifier</td>\n",
       "      <td>2020-01-30T16:22:54Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.752540</td>\n",
       "      <td>[0.12595532039976484, 0.17683374981625755, 0.2...</td>\n",
       "      <td>[[27258, 9040], [12019, 19719]]</td>\n",
       "      <td>0.690473</td>\n",
       "      <td>[0.7509504655903907, 0.6213056903396559]</td>\n",
       "      <td>[0.5335116702921983, 0.46648832970780174]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206048031</th>\n",
       "      <td>59759587</td>\n",
       "      <td>pickup_ever</td>\n",
       "      <td>sparse_logistic</td>\n",
       "      <td>2020-01-30T16:21:52Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.738766</td>\n",
       "      <td>[0.12007642563198119, 0.18903424959576656, 0.2...</td>\n",
       "      <td>[[24378, 11920], [9587, 22151]]</td>\n",
       "      <td>0.683888</td>\n",
       "      <td>[0.6716072510882142, 0.6979330770684983]</td>\n",
       "      <td>[0.5335116702921983, 0.46648832970780174]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206048041</th>\n",
       "      <td>59759594</td>\n",
       "      <td>id_first</td>\n",
       "      <td>random_forest_classifier</td>\n",
       "      <td>2020-01-30T16:22:52Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.780054</td>\n",
       "      <td>[0.033362727807172254, 0.05100690871674261, 0....</td>\n",
       "      <td>[[54041, 0], [13995, 0]]</td>\n",
       "      <td>0.794300</td>\n",
       "      <td>[1.0, 0.0]</td>\n",
       "      <td>[0.7943000764301252, 0.20569992356987477]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206048053</th>\n",
       "      <td>59759599</td>\n",
       "      <td>id_first</td>\n",
       "      <td>sparse_logistic</td>\n",
       "      <td>2020-01-30T16:21:45Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.750056</td>\n",
       "      <td>[0.04659024103468548, 0.05541672791415552, 0.0...</td>\n",
       "      <td>[[53501, 540], [13357, 638]]</td>\n",
       "      <td>0.795740</td>\n",
       "      <td>[0.99000758683222, 0.04558770989639157]</td>\n",
       "      <td>[0.7943000764301252, 0.20569992356987477]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206048061</th>\n",
       "      <td>59759609</td>\n",
       "      <td>id_last</td>\n",
       "      <td>random_forest_classifier</td>\n",
       "      <td>2020-01-30T16:22:59Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.737336</td>\n",
       "      <td>[0.058495002939447385, 0.08937233573423489, 0....</td>\n",
       "      <td>[[50007, 168], [17478, 383]]</td>\n",
       "      <td>0.740637</td>\n",
       "      <td>[0.9966517189835575, 0.021443368232461787]</td>\n",
       "      <td>[0.7374772179434417, 0.26252278205655827]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206048081</th>\n",
       "      <td>59759617</td>\n",
       "      <td>id_last</td>\n",
       "      <td>sparse_logistic</td>\n",
       "      <td>2020-01-30T16:22:27Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.705756</td>\n",
       "      <td>[0.0805408583186361, 0.10848155225635749, 0.13...</td>\n",
       "      <td>[[49003, 1172], [16278, 1583]]</td>\n",
       "      <td>0.743518</td>\n",
       "      <td>[0.9766417538614848, 0.08862885616706791]</td>\n",
       "      <td>[0.7374772179434417, 0.26252278205655827]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206048098</th>\n",
       "      <td>59759627</td>\n",
       "      <td>id_ever</td>\n",
       "      <td>random_forest_classifier</td>\n",
       "      <td>2020-01-30T16:23:04Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.734026</td>\n",
       "      <td>[0.062169312169312166, 0.11083345582831104, 0....</td>\n",
       "      <td>[[45099, 805], [20609, 1523]]</td>\n",
       "      <td>0.685255</td>\n",
       "      <td>[0.982463401882189, 0.0688143864088198]</td>\n",
       "      <td>[0.6747016285495914, 0.32529837145040863]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206048113</th>\n",
       "      <td>59759642</td>\n",
       "      <td>id_ever</td>\n",
       "      <td>sparse_logistic</td>\n",
       "      <td>2020-01-30T16:22:36Z</td>\n",
       "      <td>68036</td>\n",
       "      <td>204</td>\n",
       "      <td>0.705597</td>\n",
       "      <td>[0.0856848912404468, 0.12979567837718653, 0.17...</td>\n",
       "      <td>[[42230, 3674], [17488, 4644]]</td>\n",
       "      <td>0.688959</td>\n",
       "      <td>[0.919963401882189, 0.2098319175853967]</td>\n",
       "      <td>[0.6747016285495914, 0.32529837145040863]</td>\n",
       "      <td>[civis_2020_marriage, civis_2020_children_pres...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             job_id            dv                     model  \\\n",
       "run_id                                                        \n",
       "206047980  59759537  pickup_first  random_forest_classifier   \n",
       "206047993  59759544  pickup_first           sparse_logistic   \n",
       "206047997  59759550   pickup_last  random_forest_classifier   \n",
       "206048012  59759567   pickup_last           sparse_logistic   \n",
       "206048016  59759573   pickup_ever  random_forest_classifier   \n",
       "206048031  59759587   pickup_ever           sparse_logistic   \n",
       "206048041  59759594      id_first  random_forest_classifier   \n",
       "206048053  59759599      id_first           sparse_logistic   \n",
       "206048061  59759609       id_last  random_forest_classifier   \n",
       "206048081  59759617       id_last           sparse_logistic   \n",
       "206048098  59759627       id_ever  random_forest_classifier   \n",
       "206048113  59759642       id_ever           sparse_logistic   \n",
       "\n",
       "              time_of_train_run  n_rows  n_features       auc  \\\n",
       "run_id                                                          \n",
       "206047980  2020-01-30T16:22:35Z   68036         204  0.769170   \n",
       "206047993  2020-01-30T16:22:02Z   68036         204  0.757209   \n",
       "206047997  2020-01-30T16:22:42Z   68036         204  0.752649   \n",
       "206048012  2020-01-30T16:22:07Z   68036         204  0.745124   \n",
       "206048016  2020-01-30T16:22:54Z   68036         204  0.752540   \n",
       "206048031  2020-01-30T16:21:52Z   68036         204  0.738766   \n",
       "206048041  2020-01-30T16:22:52Z   68036         204  0.780054   \n",
       "206048053  2020-01-30T16:21:45Z   68036         204  0.750056   \n",
       "206048061  2020-01-30T16:22:59Z   68036         204  0.737336   \n",
       "206048081  2020-01-30T16:22:27Z   68036         204  0.705756   \n",
       "206048098  2020-01-30T16:23:04Z   68036         204  0.734026   \n",
       "206048113  2020-01-30T16:22:36Z   68036         204  0.705597   \n",
       "\n",
       "                                                     deciles  \\\n",
       "run_id                                                         \n",
       "206047980  [0.07304526748971193, 0.11215640158753491, 0.1...   \n",
       "206047993  [0.08597883597883597, 0.10157283551374394, 0.1...   \n",
       "206047997  [0.09082892416225749, 0.1306776422166691, 0.18...   \n",
       "206048012  [0.09141681363903587, 0.12803175069822137, 0.1...   \n",
       "206048016  [0.12595532039976484, 0.17683374981625755, 0.2...   \n",
       "206048031  [0.12007642563198119, 0.18903424959576656, 0.2...   \n",
       "206048041  [0.033362727807172254, 0.05100690871674261, 0....   \n",
       "206048053  [0.04659024103468548, 0.05541672791415552, 0.0...   \n",
       "206048061  [0.058495002939447385, 0.08937233573423489, 0....   \n",
       "206048081  [0.0805408583186361, 0.10848155225635749, 0.13...   \n",
       "206048098  [0.062169312169312166, 0.11083345582831104, 0....   \n",
       "206048113  [0.0856848912404468, 0.12979567837718653, 0.17...   \n",
       "\n",
       "                          confusion_matrix  accuracy  \\\n",
       "run_id                                                 \n",
       "206047980    [[46731, 581], [19211, 1513]]  0.709095   \n",
       "206047993   [[42357, 4955], [12859, 7865]]  0.738168   \n",
       "206047997   [[42671, 1037], [21519, 2809]]  0.668470   \n",
       "206048012  [[36374, 7334], [12840, 11488]]  0.703481   \n",
       "206048016  [[27258, 9040], [12019, 19719]]  0.690473   \n",
       "206048031  [[24378, 11920], [9587, 22151]]  0.683888   \n",
       "206048041         [[54041, 0], [13995, 0]]  0.794300   \n",
       "206048053     [[53501, 540], [13357, 638]]  0.795740   \n",
       "206048061     [[50007, 168], [17478, 383]]  0.740637   \n",
       "206048081   [[49003, 1172], [16278, 1583]]  0.743518   \n",
       "206048098    [[45099, 805], [20609, 1523]]  0.685255   \n",
       "206048113   [[42230, 3674], [17488, 4644]]  0.688959   \n",
       "\n",
       "                                            p_correct  \\\n",
       "run_id                                                  \n",
       "206047980   [0.9877198173824823, 0.07300714147847906]   \n",
       "206047993   [0.8952696990192763, 0.37951167728237795]   \n",
       "206047997   [0.9762743662487416, 0.11546366326866163]   \n",
       "206048012   [0.8322046307312162, 0.47221308780006577]   \n",
       "206048016    [0.7509504655903907, 0.6213056903396559]   \n",
       "206048031    [0.6716072510882142, 0.6979330770684983]   \n",
       "206048041                                  [1.0, 0.0]   \n",
       "206048053     [0.99000758683222, 0.04558770989639157]   \n",
       "206048061  [0.9966517189835575, 0.021443368232461787]   \n",
       "206048081   [0.9766417538614848, 0.08862885616706791]   \n",
       "206048098     [0.982463401882189, 0.0688143864088198]   \n",
       "206048113     [0.919963401882189, 0.2098319175853967]   \n",
       "\n",
       "                                  pop_incidence_true  \\\n",
       "run_id                                                 \n",
       "206047980  [0.6953965547651243, 0.30460344523487565]   \n",
       "206047993  [0.6953965547651243, 0.30460344523487565]   \n",
       "206047997  [0.6424245987418425, 0.35757540125815745]   \n",
       "206048012  [0.6424245987418425, 0.35757540125815745]   \n",
       "206048016  [0.5335116702921983, 0.46648832970780174]   \n",
       "206048031  [0.5335116702921983, 0.46648832970780174]   \n",
       "206048041  [0.7943000764301252, 0.20569992356987477]   \n",
       "206048053  [0.7943000764301252, 0.20569992356987477]   \n",
       "206048061  [0.7374772179434417, 0.26252278205655827]   \n",
       "206048081  [0.7374772179434417, 0.26252278205655827]   \n",
       "206048098  [0.6747016285495914, 0.32529837145040863]   \n",
       "206048113  [0.6747016285495914, 0.32529837145040863]   \n",
       "\n",
       "                                                feature_list  \n",
       "run_id                                                        \n",
       "206047980  [civis_2020_marriage, civis_2020_children_pres...  \n",
       "206047993  [civis_2020_children_present, civis_2020_spani...  \n",
       "206047997  [civis_2020_marriage, civis_2020_children_pres...  \n",
       "206048012  [civis_2020_marriage, civis_2020_children_pres...  \n",
       "206048016  [civis_2020_marriage, civis_2020_children_pres...  \n",
       "206048031  [civis_2020_marriage, civis_2020_children_pres...  \n",
       "206048041  [civis_2020_marriage, civis_2020_children_pres...  \n",
       "206048053  [civis_2020_marriage, civis_2020_children_pres...  \n",
       "206048061  [civis_2020_marriage, civis_2020_children_pres...  \n",
       "206048081  [civis_2020_marriage, civis_2020_children_pres...  \n",
       "206048098  [civis_2020_marriage, civis_2020_children_pres...  \n",
       "206048113  [civis_2020_marriage, civis_2020_children_pres...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate validation metrics\n",
    "metrics_list = []\n",
    "\n",
    "for b in train_output:\n",
    "    if b.job_id != 59761951:\n",
    "        metric = {'job_id':b.job_id,\n",
    "                  'run_id':b.run_id,\n",
    "                  'dv': ''.join(b.metadata['run']['configuration']['data']['y']),\n",
    "                  'model': b.metadata['model']['model'],\n",
    "                  'time_of_train_run': b.metadata['run']['time_of_run'],\n",
    "                  'n_rows': b.metadata['data']['n_rows'],\n",
    "                  'n_features': b.metadata['data']['n_cols'],\n",
    "                  'auc': b.metadata['metrics']['roc_auc'],\n",
    "                  'deciles': b.metadata['metrics']['deciles'],\n",
    "                  'confusion_matrix': b.metadata['metrics']['confusion_matrix'],\n",
    "                  'accuracy': b.metadata['metrics']['accuracy'],\n",
    "                  'p_correct': b.metadata['metrics']['p_correct'],\n",
    "                  'pop_incidence_true': b.metadata['metrics']['pop_incidence_true'],\n",
    "                  'feature_list':b.metadata['model']['parameters']['relvars']\n",
    "                 }\n",
    "        metrics_list.append(metric)\n",
    "    \n",
    "metric_order = (['job_id', 'run_id', 'dv', 'model', 'time_of_train_run', 'n_rows', 'n_features',\n",
    "                 'auc', 'deciles', 'confusion_matrix', 'accuracy', 'p_correct','pop_incidence_true','feature_list'])\n",
    "\n",
    "validation_df = pd.DataFrame.from_records(metrics_list, columns=metric_order, index='run_id')\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write validation metrics to Redshift\n",
    "create_validation_table = civis.io.dataframe_to_civis(df=validation_df,\n",
    "                                                 database=DATABASE, \n",
    "                                                 table= f'{SCHEMA}.{PREFIX}_validation_{datestamp}', \n",
    "                                                 existing_table_rows='drop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the voterfile\n",
    "scores_list = []\n",
    "for m,t in zip(model_output, train_output):\n",
    "    DV_NAME = ''.join(t.metadata['run']['configuration']['data']['y'])\n",
    "    print(DV_NAME)\n",
    "    SCORES_TABLE = f'{SCHEMA}.{PREFIX}_{DV_NAME}_{datestamp}'\n",
    "    scores_list.append(SCORES_TABLE)\n",
    "    scores = m.predict(primary_key=PRIMARY_KEY,\n",
    "                       database_name=DATABASE, \n",
    "                       table_name=MODELING_FRAME,\n",
    "                       if_exists='drop',\n",
    "                       output_table=SCORES_TABLE,\n",
    "                       disk_space=10)\n",
    "scores.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate SQL for final output table and drop intermediary tables\n",
    "input_train_list = []\n",
    "output_score_list = []\n",
    "for i in range(len(DV_LIST)):\n",
    "    input_train = f\"{SCHEMA}.{PREFIX}_training_{i}\"\n",
    "    input_train_list.append(input_train)\n",
    "    output_score = f\"{SCHEMA}.{PREFIX}_{DV_LIST[i]}_{datestamp}\"\n",
    "    output_score_list.append(output_score)\n",
    "\n",
    "drop_input_train_sql = \"\\n\".join([\"drop table if exists {tbl};\".format(tbl=v) for v in input_train_list])\n",
    "drop_output_score_sql = \"\\n\".join([\"drop table if exists {tbl};\".format(tbl=t) for t in output_score_list])  \n",
    "dv_strings = \"\\n,\".join([\"{dv_score}_1 as {dv_score}\".format(dv_score=dv) for dv in DV_LIST])\n",
    "dv_tiles = \"\\n,\".join([\"NTILE(100) OVER (ORDER BY {dv_tile}_1) AS {dv_tile}_100\".format(dv_tile=dv) for dv in DV_LIST])\n",
    "join_table = []\n",
    "if len(output_score_list) > 1:\n",
    "    for i in output_score_list[1:]:\n",
    "        j = str(' left join '+f'{i}'+f' using({PRIMARY_KEY}) ')\n",
    "        join_table.append(j)\n",
    "        #dv_strings = \"\\nleft join \".join([\"{dv_score}\".format(table=tbl) for tbl in table_list[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_table_sql = f\"\"\"\n",
    "set query_group to 'importers';\n",
    "set wlm_query_slot_count to 3;\n",
    "DROP TABLE IF EXISTS {SCHEMA}.{PREFIX}_output_{datestamp};\n",
    "CREATE TABLE {SCHEMA}.{PREFIX}_output_{datestamp}\n",
    "  DISTSTYLE KEY\n",
    "  DISTKEY ({PRIMARY_KEY})\n",
    "  SORTKEY ({PRIMARY_KEY})\n",
    "  AS (\"\"\"+'select '+ f\"{PRIMARY_KEY} \\n,\" + dv_strings + \"\\n,\" + dv_tiles + ' from '+ ''.join(output_score_list[0]) + ''.join(join_table) +');'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final output table\n",
    "create_output_table = civis.io.query_civis(sql=output_table_sql, database=DATABASE)\n",
    "create_output_table.result().state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop intermediary tables\n",
    "drop_input_train_query = civis.io.query_civis(sql=drop_input_train_sql, database=DATABASE)\n",
    "drop_input_train_query.result().state\n",
    "\n",
    "drop_output_score_query = civis.io.query_civis(sql=drop_output_score_sql, database=DATABASE)\n",
    "drop_output_score_query.result().state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(drop_input_train_sql)\n",
    "print(drop_output_score_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grant team on tables\n",
    "grant_statement = f\"\"\"\n",
    "GRANT ALL ON SCHEMA {SCHEMA} TO GROUP bernie_data;\n",
    "GRANT SELECT ON {SCHEMA}.{PREFIX}_output_{datestamp} TO GROUP bernie_data;\n",
    "GRANT SELECT ON {SCHEMA}.{PREFIX}_validation_{datestamp} TO GROUP bernie_data;\n",
    "\"\"\"\n",
    "grant_team = civis.io.query_civis(sql=grant_statement, database=DATABASE)\n",
    "grant_team.result().state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grant_statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
